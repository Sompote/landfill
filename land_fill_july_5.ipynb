{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HIFs8I_eA73m",
    "outputId": "56efd925-c57b-4c52-eb06-fb909c34d0c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:1, Batch:    5] loss: 0.004824\n",
      "[Epoch:1] Test Loss: 0.137364, MAPE: 1180593029120.000\n",
      "[Epoch:2, Batch:    5] loss: 0.004570\n",
      "[Epoch:2] Test Loss: 0.137364, MAPE: 1180574679040.000\n",
      "[Epoch:3, Batch:    5] loss: 0.004373\n",
      "[Epoch:3] Test Loss: 0.137364, MAPE: 1180583067648.000\n",
      "[Epoch:4, Batch:    5] loss: 0.003699\n",
      "[Epoch:4] Test Loss: 0.137364, MAPE: 1180581625856.000\n",
      "[Epoch:5, Batch:    5] loss: 0.004687\n",
      "[Epoch:5] Test Loss: 0.137364, MAPE: 1180547678208.000\n",
      "[Epoch:6, Batch:    5] loss: 0.005533\n",
      "[Epoch:6] Test Loss: 0.137364, MAPE: 1180519366656.000\n",
      "[Epoch:7, Batch:    5] loss: 0.005206\n",
      "[Epoch:7] Test Loss: 0.137364, MAPE: 1180511895552.000\n",
      "[Epoch:8, Batch:    5] loss: 0.004873\n",
      "[Epoch:8] Test Loss: 0.137364, MAPE: 1180511764480.000\n",
      "[Epoch:9, Batch:    5] loss: 0.005274\n",
      "[Epoch:9] Test Loss: 0.137364, MAPE: 1180527362048.000\n",
      "[Epoch:10, Batch:    5] loss: 0.004714\n",
      "[Epoch:10] Test Loss: 0.137364, MAPE: 1180531687424.000\n",
      "[Epoch:11, Batch:    5] loss: 0.003706\n",
      "[Epoch:11] Test Loss: 0.137364, MAPE: 1180551217152.000\n",
      "[Epoch:12, Batch:    5] loss: 0.004463\n",
      "[Epoch:12] Test Loss: 0.137364, MAPE: 1180573106176.000\n",
      "[Epoch:13, Batch:    5] loss: 0.005075\n",
      "[Epoch:13] Test Loss: 0.137364, MAPE: 1180597747712.000\n",
      "[Epoch:14, Batch:    5] loss: 0.005402\n",
      "[Epoch:14] Test Loss: 0.137364, MAPE: 1180621733888.000\n",
      "[Epoch:15, Batch:    5] loss: 0.006232\n",
      "[Epoch:15] Test Loss: 0.137364, MAPE: 1180667740160.000\n",
      "[Epoch:16, Batch:    5] loss: 0.005743\n",
      "[Epoch:16] Test Loss: 0.137364, MAPE: 1180686221312.000\n",
      "[Epoch:17, Batch:    5] loss: 0.004423\n",
      "[Epoch:17] Test Loss: 0.137364, MAPE: 1180704702464.000\n",
      "[Epoch:18, Batch:    5] loss: 0.004747\n",
      "[Epoch:18] Test Loss: 0.137364, MAPE: 1180730523648.000\n",
      "[Epoch:19, Batch:    5] loss: 0.005231\n",
      "[Epoch:19] Test Loss: 0.137364, MAPE: 1180764078080.000\n",
      "[Epoch:20, Batch:    5] loss: 0.006488\n",
      "[Epoch:20] Test Loss: 0.137364, MAPE: 1180785049600.000\n",
      "[Epoch:21, Batch:    5] loss: 0.004131\n",
      "[Epoch:21] Test Loss: 0.137364, MAPE: 1180795273216.000\n",
      "[Epoch:22, Batch:    5] loss: 0.006919\n",
      "[Epoch:22] Test Loss: 0.137364, MAPE: 1180816375808.000\n",
      "[Epoch:23, Batch:    5] loss: 0.003718\n",
      "[Epoch:23] Test Loss: 0.137364, MAPE: 1180856877056.000\n",
      "[Epoch:24, Batch:    5] loss: 0.004687\n",
      "[Epoch:24] Test Loss: 0.137364, MAPE: 1180879814656.000\n",
      "[Epoch:25, Batch:    5] loss: 0.005313\n",
      "[Epoch:25] Test Loss: 0.137364, MAPE: 1180918874112.000\n",
      "[Epoch:26, Batch:    5] loss: 0.003333\n",
      "[Epoch:26] Test Loss: 0.137364, MAPE: 1180978380800.000\n",
      "[Epoch:27, Batch:    5] loss: 0.003840\n",
      "[Epoch:27] Test Loss: 0.137364, MAPE: 1181030940672.000\n",
      "[Epoch:28, Batch:    5] loss: 0.003416\n",
      "[Epoch:28] Test Loss: 0.137364, MAPE: 1181108142080.000\n",
      "[Epoch:29, Batch:    5] loss: 0.005343\n",
      "[Epoch:29] Test Loss: 0.137364, MAPE: 1181168041984.000\n",
      "[Epoch:30, Batch:    5] loss: 0.003929\n",
      "[Epoch:30] Test Loss: 0.137364, MAPE: 1181236854784.000\n",
      "[Epoch:31, Batch:    5] loss: 0.004497\n",
      "[Epoch:31] Test Loss: 0.137364, MAPE: 1181322444800.000\n",
      "[Epoch:32, Batch:    5] loss: 0.005205\n",
      "[Epoch:32] Test Loss: 0.137364, MAPE: 1181403578368.000\n",
      "[Epoch:33, Batch:    5] loss: 0.005886\n",
      "[Epoch:33] Test Loss: 0.137364, MAPE: 1181517742080.000\n",
      "[Epoch:34, Batch:    5] loss: 0.004405\n",
      "[Epoch:34] Test Loss: 0.137364, MAPE: 1181647110144.000\n",
      "[Epoch:35, Batch:    5] loss: 0.004521\n",
      "[Epoch:35] Test Loss: 0.137364, MAPE: 1181777788928.000\n",
      "[Epoch:36, Batch:    5] loss: 0.006074\n",
      "[Epoch:36] Test Loss: 0.137364, MAPE: 1181901520896.000\n",
      "[Epoch:37, Batch:    5] loss: 0.004044\n",
      "[Epoch:37] Test Loss: 0.137364, MAPE: 1182031937536.000\n",
      "[Epoch:38, Batch:    5] loss: 0.004191\n",
      "[Epoch:38] Test Loss: 0.137364, MAPE: 1182251745280.000\n",
      "[Epoch:39, Batch:    5] loss: 0.004275\n",
      "[Epoch:39] Test Loss: 0.137364, MAPE: 1182404968448.000\n",
      "[Epoch:40, Batch:    5] loss: 0.004709\n",
      "[Epoch:40] Test Loss: 0.137364, MAPE: 1182573527040.000\n",
      "[Epoch:41, Batch:    5] loss: 0.004593\n",
      "[Epoch:41] Test Loss: 0.137364, MAPE: 1182779179008.000\n",
      "[Epoch:42, Batch:    5] loss: 0.003835\n",
      "[Epoch:42] Test Loss: 0.137364, MAPE: 1182936334336.000\n",
      "[Epoch:43, Batch:    5] loss: 0.005572\n",
      "[Epoch:43] Test Loss: 0.137364, MAPE: 1183165710336.000\n",
      "[Epoch:44, Batch:    5] loss: 0.004676\n",
      "[Epoch:44] Test Loss: 0.137364, MAPE: 1183401508864.000\n",
      "[Epoch:45, Batch:    5] loss: 0.004399\n",
      "[Epoch:45] Test Loss: 0.137364, MAPE: 1183695110144.000\n",
      "[Epoch:46, Batch:    5] loss: 0.005203\n",
      "[Epoch:46] Test Loss: 0.137364, MAPE: 1183923830784.000\n",
      "[Epoch:47, Batch:    5] loss: 0.004700\n",
      "[Epoch:47] Test Loss: 0.137364, MAPE: 1184092520448.000\n",
      "[Epoch:48, Batch:    5] loss: 0.004408\n",
      "[Epoch:48] Test Loss: 0.137364, MAPE: 1184299483136.000\n",
      "[Epoch:49, Batch:    5] loss: 0.005860\n",
      "[Epoch:49] Test Loss: 0.137364, MAPE: 1184552845312.000\n",
      "[Epoch:50, Batch:    5] loss: 0.005066\n",
      "[Epoch:50] Test Loss: 0.137364, MAPE: 1184797949952.000\n",
      "[Epoch:51, Batch:    5] loss: 0.004443\n",
      "[Epoch:51] Test Loss: 0.137364, MAPE: 1184986038272.000\n",
      "[Epoch:52, Batch:    5] loss: 0.004905\n",
      "[Epoch:52] Test Loss: 0.137364, MAPE: 1185157873664.000\n",
      "[Epoch:53, Batch:    5] loss: 0.005033\n",
      "[Epoch:53] Test Loss: 0.137364, MAPE: 1185477296128.000\n",
      "[Epoch:54, Batch:    5] loss: 0.004719\n",
      "[Epoch:54] Test Loss: 0.137364, MAPE: 1185711521792.000\n",
      "[Epoch:55, Batch:    5] loss: 0.004433\n",
      "[Epoch:55] Test Loss: 0.137364, MAPE: 1185994637312.000\n",
      "[Epoch:56, Batch:    5] loss: 0.003777\n",
      "[Epoch:56] Test Loss: 0.137364, MAPE: 1186426650624.000\n",
      "[Epoch:57, Batch:    5] loss: 0.005103\n",
      "[Epoch:57] Test Loss: 0.137364, MAPE: 1186615656448.000\n",
      "[Epoch:58, Batch:    5] loss: 0.003967\n",
      "[Epoch:58] Test Loss: 0.137364, MAPE: 1186847784960.000\n",
      "[Epoch:59, Batch:    5] loss: 0.005941\n",
      "[Epoch:59] Test Loss: 0.137364, MAPE: 1187095379968.000\n",
      "[Epoch:60, Batch:    5] loss: 0.004830\n",
      "[Epoch:60] Test Loss: 0.137364, MAPE: 1187304177664.000\n",
      "[Epoch:61, Batch:    5] loss: 0.003781\n",
      "[Epoch:61] Test Loss: 0.137364, MAPE: 1187612196864.000\n",
      "[Epoch:62, Batch:    5] loss: 0.005506\n",
      "[Epoch:62] Test Loss: 0.137364, MAPE: 1187770793984.000\n",
      "[Epoch:63, Batch:    5] loss: 0.003693\n",
      "[Epoch:63] Test Loss: 0.137364, MAPE: 1187917201408.000\n",
      "[Epoch:64, Batch:    5] loss: 0.004286\n",
      "[Epoch:64] Test Loss: 0.137364, MAPE: 1188092444672.000\n",
      "[Epoch:65, Batch:    5] loss: 0.004956\n",
      "[Epoch:65] Test Loss: 0.137365, MAPE: 1188405837824.000\n",
      "[Epoch:66, Batch:    5] loss: 0.004470\n",
      "[Epoch:66] Test Loss: 0.137365, MAPE: 1188750295040.000\n",
      "[Epoch:67, Batch:    5] loss: 0.003966\n",
      "[Epoch:67] Test Loss: 0.137365, MAPE: 1188902338560.000\n",
      "[Epoch:68, Batch:    5] loss: 0.005719\n",
      "[Epoch:68] Test Loss: 0.137365, MAPE: 1189145477120.000\n",
      "[Epoch:69, Batch:    5] loss: 0.005387\n",
      "[Epoch:69] Test Loss: 0.137365, MAPE: 1189344313344.000\n",
      "[Epoch:70, Batch:    5] loss: 0.004214\n",
      "[Epoch:70] Test Loss: 0.137365, MAPE: 1189514051584.000\n",
      "[Epoch:71, Batch:    5] loss: 0.005388\n",
      "[Epoch:71] Test Loss: 0.137365, MAPE: 1189761777664.000\n",
      "[Epoch:72, Batch:    5] loss: 0.004689\n",
      "[Epoch:72] Test Loss: 0.137365, MAPE: 1190005571584.000\n",
      "[Epoch:73, Batch:    5] loss: 0.004768\n",
      "[Epoch:73] Test Loss: 0.137365, MAPE: 1189995347968.000\n",
      "[Epoch:74, Batch:    5] loss: 0.005048\n",
      "[Epoch:74] Test Loss: 0.137365, MAPE: 1190090113024.000\n",
      "[Epoch:75, Batch:    5] loss: 0.004826\n",
      "[Epoch:75] Test Loss: 0.137365, MAPE: 1190238355456.000\n",
      "[Epoch:76, Batch:    5] loss: 0.005227\n",
      "[Epoch:76] Test Loss: 0.137365, MAPE: 1190518849536.000\n",
      "[Epoch:77, Batch:    5] loss: 0.005071\n",
      "[Epoch:77] Test Loss: 0.137365, MAPE: 1190618202112.000\n",
      "[Epoch:78, Batch:    5] loss: 0.003467\n",
      "[Epoch:78] Test Loss: 0.137365, MAPE: 1190758842368.000\n",
      "[Epoch:79, Batch:    5] loss: 0.004587\n",
      "[Epoch:79] Test Loss: 0.137365, MAPE: 1190963707904.000\n",
      "[Epoch:80, Batch:    5] loss: 0.005436\n",
      "[Epoch:80] Test Loss: 0.137365, MAPE: 1191395459072.000\n",
      "[Epoch:81, Batch:    5] loss: 0.006147\n",
      "[Epoch:81] Test Loss: 0.137365, MAPE: 1191529676800.000\n",
      "[Epoch:82, Batch:    5] loss: 0.004957\n",
      "[Epoch:82] Test Loss: 0.137365, MAPE: 1191481311232.000\n",
      "[Epoch:83, Batch:    5] loss: 0.004496\n",
      "[Epoch:83] Test Loss: 0.137365, MAPE: 1191685521408.000\n",
      "[Epoch:84, Batch:    5] loss: 0.005388\n",
      "[Epoch:84] Test Loss: 0.137366, MAPE: 1191740964864.000\n",
      "[Epoch:85, Batch:    5] loss: 0.004127\n",
      "[Epoch:85] Test Loss: 0.137366, MAPE: 1191876886528.000\n",
      "[Epoch:86, Batch:    5] loss: 0.004938\n",
      "[Epoch:86] Test Loss: 0.137366, MAPE: 1191888289792.000\n",
      "[Epoch:87, Batch:    5] loss: 0.002939\n",
      "[Epoch:87] Test Loss: 0.137366, MAPE: 1191895367680.000\n",
      "[Epoch:88, Batch:    5] loss: 0.004238\n",
      "[Epoch:88] Test Loss: 0.137366, MAPE: 1192067989504.000\n",
      "[Epoch:89, Batch:    5] loss: 0.004649\n",
      "[Epoch:89] Test Loss: 0.137366, MAPE: 1192012546048.000\n",
      "[Epoch:90, Batch:    5] loss: 0.004283\n",
      "[Epoch:90] Test Loss: 0.137366, MAPE: 1192258961408.000\n",
      "[Epoch:91, Batch:    5] loss: 0.005312\n",
      "[Epoch:91] Test Loss: 0.137366, MAPE: 1192215052288.000\n",
      "[Epoch:92, Batch:    5] loss: 0.005448\n",
      "[Epoch:92] Test Loss: 0.137366, MAPE: 1192394883072.000\n",
      "[Epoch:93, Batch:    5] loss: 0.003939\n",
      "[Epoch:93] Test Loss: 0.137366, MAPE: 1192286486528.000\n",
      "[Epoch:94, Batch:    5] loss: 0.005001\n",
      "[Epoch:94] Test Loss: 0.137366, MAPE: 1192526479360.000\n",
      "[Epoch:95, Batch:    5] loss: 0.004289\n",
      "[Epoch:95] Test Loss: 0.137366, MAPE: 1192637235200.000\n",
      "[Epoch:96, Batch:    5] loss: 0.004448\n",
      "[Epoch:96] Test Loss: 0.137366, MAPE: 1192751136768.000\n",
      "[Epoch:97, Batch:    5] loss: 0.005518\n",
      "[Epoch:97] Test Loss: 0.137366, MAPE: 1192697266176.000\n",
      "[Epoch:98, Batch:    5] loss: 0.005694\n",
      "[Epoch:98] Test Loss: 0.137366, MAPE: 1192759525376.000\n",
      "[Epoch:99, Batch:    5] loss: 0.004044\n",
      "[Epoch:99] Test Loss: 0.137366, MAPE: 1192785608704.000\n",
      "[Epoch:100, Batch:    5] loss: 0.004724\n",
      "[Epoch:100] Test Loss: 0.137366, MAPE: 1193135046656.000\n",
      "[Epoch:101, Batch:    5] loss: 0.005421\n",
      "[Epoch:101] Test Loss: 0.137366, MAPE: 1193033990144.000\n",
      "[Epoch:102, Batch:    5] loss: 0.004233\n",
      "[Epoch:102] Test Loss: 0.137366, MAPE: 1193135046656.000\n",
      "[Epoch:103, Batch:    5] loss: 0.006005\n",
      "[Epoch:103] Test Loss: 0.137366, MAPE: 1192960851968.000\n",
      "[Epoch:104, Batch:    5] loss: 0.005133\n",
      "[Epoch:104] Test Loss: 0.137366, MAPE: 1193091923968.000\n",
      "[Epoch:105, Batch:    5] loss: 0.005129\n",
      "[Epoch:105] Test Loss: 0.137366, MAPE: 1193176465408.000\n",
      "[Epoch:106, Batch:    5] loss: 0.005354\n",
      "[Epoch:106] Test Loss: 0.137366, MAPE: 1193301245952.000\n",
      "[Epoch:107, Batch:    5] loss: 0.003743\n",
      "[Epoch:107] Test Loss: 0.137366, MAPE: 1193333882880.000\n",
      "[Epoch:108, Batch:    5] loss: 0.005134\n",
      "[Epoch:108] Test Loss: 0.137366, MAPE: 1193392865280.000\n",
      "[Epoch:109, Batch:    5] loss: 0.003547\n",
      "[Epoch:109] Test Loss: 0.137366, MAPE: 1193278701568.000\n",
      "[Epoch:110, Batch:    5] loss: 0.004928\n",
      "[Epoch:110] Test Loss: 0.137366, MAPE: 1193472557056.000\n",
      "[Epoch:111, Batch:    5] loss: 0.005094\n",
      "[Epoch:111] Test Loss: 0.137366, MAPE: 1193439395840.000\n",
      "[Epoch:112, Batch:    5] loss: 0.004722\n",
      "[Epoch:112] Test Loss: 0.137366, MAPE: 1193660776448.000\n",
      "[Epoch:113, Batch:    5] loss: 0.004368\n",
      "[Epoch:113] Test Loss: 0.137366, MAPE: 1193532588032.000\n",
      "[Epoch:114, Batch:    5] loss: 0.004705\n",
      "[Epoch:114] Test Loss: 0.137366, MAPE: 1193702195200.000\n",
      "[Epoch:115, Batch:    5] loss: 0.004230\n",
      "[Epoch:115] Test Loss: 0.137366, MAPE: 1193639018496.000\n",
      "[Epoch:116, Batch:    5] loss: 0.003345\n",
      "[Epoch:116] Test Loss: 0.137366, MAPE: 1193680699392.000\n",
      "[Epoch:117, Batch:    5] loss: 0.004840\n",
      "[Epoch:117] Test Loss: 0.137366, MAPE: 1193756983296.000\n",
      "[Epoch:118, Batch:    5] loss: 0.005561\n",
      "[Epoch:118] Test Loss: 0.137366, MAPE: 1193546874880.000\n",
      "[Epoch:119, Batch:    5] loss: 0.005007\n",
      "[Epoch:119] Test Loss: 0.137366, MAPE: 1193582395392.000\n",
      "[Epoch:120, Batch:    5] loss: 0.005047\n",
      "[Epoch:120] Test Loss: 0.137366, MAPE: 1193755410432.000\n",
      "[Epoch:121, Batch:    5] loss: 0.004322\n",
      "[Epoch:121] Test Loss: 0.137366, MAPE: 1193738502144.000\n",
      "[Epoch:122, Batch:    5] loss: 0.004364\n",
      "[Epoch:122] Test Loss: 0.137366, MAPE: 1193340174336.000\n",
      "[Epoch:123, Batch:    5] loss: 0.005779\n",
      "[Epoch:123] Test Loss: 0.137366, MAPE: 1193470459904.000\n",
      "[Epoch:124, Batch:    5] loss: 0.004213\n",
      "[Epoch:124] Test Loss: 0.137366, MAPE: 1193673490432.000\n",
      "[Epoch:125, Batch:    5] loss: 0.005767\n",
      "[Epoch:125] Test Loss: 0.137366, MAPE: 1193811247104.000\n",
      "[Epoch:126, Batch:    5] loss: 0.004648\n",
      "[Epoch:126] Test Loss: 0.137366, MAPE: 1193664053248.000\n",
      "[Epoch:127, Batch:    5] loss: 0.004681\n",
      "[Epoch:127] Test Loss: 0.137366, MAPE: 1193398763520.000\n",
      "[Epoch:128, Batch:    5] loss: 0.005244\n",
      "[Epoch:128] Test Loss: 0.137366, MAPE: 1193376612352.000\n",
      "[Epoch:129, Batch:    5] loss: 0.005106\n",
      "[Epoch:129] Test Loss: 0.137366, MAPE: 1193706258432.000\n",
      "[Epoch:130, Batch:    5] loss: 0.004504\n",
      "[Epoch:130] Test Loss: 0.137366, MAPE: 1193843490816.000\n",
      "[Epoch:131, Batch:    5] loss: 0.005616\n",
      "[Epoch:131] Test Loss: 0.137366, MAPE: 1194123853824.000\n",
      "[Epoch:132, Batch:    5] loss: 0.004734\n",
      "[Epoch:132] Test Loss: 0.137366, MAPE: 1194091216896.000\n",
      "[Epoch:133, Batch:    5] loss: 0.005117\n",
      "[Epoch:133] Test Loss: 0.137366, MAPE: 1193999597568.000\n",
      "[Epoch:134, Batch:    5] loss: 0.004256\n",
      "[Epoch:134] Test Loss: 0.137366, MAPE: 1193677553664.000\n",
      "[Epoch:135, Batch:    5] loss: 0.005739\n",
      "[Epoch:135] Test Loss: 0.137366, MAPE: 1193701670912.000\n",
      "[Epoch:136, Batch:    5] loss: 0.003418\n",
      "[Epoch:136] Test Loss: 0.137366, MAPE: 1193428123648.000\n",
      "[Epoch:137, Batch:    5] loss: 0.004986\n",
      "[Epoch:137] Test Loss: 0.137366, MAPE: 1193560899584.000\n",
      "[Epoch:138, Batch:    5] loss: 0.005339\n",
      "[Epoch:138] Test Loss: 0.137366, MAPE: 1193681616896.000\n",
      "[Epoch:139, Batch:    5] loss: 0.005226\n",
      "[Epoch:139] Test Loss: 0.137366, MAPE: 1193483698176.000\n",
      "[Epoch:140, Batch:    5] loss: 0.004109\n",
      "[Epoch:140] Test Loss: 0.137366, MAPE: 1193455386624.000\n",
      "[Epoch:141, Batch:    5] loss: 0.005569\n",
      "[Epoch:141] Test Loss: 0.137366, MAPE: 1193350135808.000\n",
      "[Epoch:142, Batch:    5] loss: 0.003372\n",
      "[Epoch:142] Test Loss: 0.137366, MAPE: 1193252618240.000\n",
      "[Epoch:143, Batch:    5] loss: 0.004488\n",
      "[Epoch:143] Test Loss: 0.137366, MAPE: 1193191669760.000\n",
      "[Epoch:144, Batch:    5] loss: 0.004919\n",
      "[Epoch:144] Test Loss: 0.137366, MAPE: 1193067806720.000\n",
      "[Epoch:145, Batch:    5] loss: 0.004208\n",
      "[Epoch:145] Test Loss: 0.137366, MAPE: 1192890728448.000\n",
      "[Epoch:146, Batch:    5] loss: 0.004728\n",
      "[Epoch:146] Test Loss: 0.137366, MAPE: 1192840134656.000\n",
      "[Epoch:147, Batch:    5] loss: 0.003739\n",
      "[Epoch:147] Test Loss: 0.137366, MAPE: 1192909078528.000\n",
      "[Epoch:148, Batch:    5] loss: 0.005022\n",
      "[Epoch:148] Test Loss: 0.137366, MAPE: 1192960065536.000\n",
      "[Epoch:149, Batch:    5] loss: 0.005163\n",
      "[Epoch:149] Test Loss: 0.137366, MAPE: 1193081044992.000\n",
      "[Epoch:150, Batch:    5] loss: 0.005632\n",
      "[Epoch:150] Test Loss: 0.137366, MAPE: 1192935424000.000\n",
      "[Epoch:151, Batch:    5] loss: 0.004538\n",
      "[Epoch:151] Test Loss: 0.137366, MAPE: 1193047097344.000\n",
      "[Epoch:152, Batch:    5] loss: 0.005679\n",
      "[Epoch:152] Test Loss: 0.137366, MAPE: 1193220767744.000\n",
      "[Epoch:153, Batch:    5] loss: 0.005427\n",
      "[Epoch:153] Test Loss: 0.137366, MAPE: 1193294299136.000\n",
      "[Epoch:154, Batch:    5] loss: 0.003500\n",
      "[Epoch:154] Test Loss: 0.137366, MAPE: 1193169387520.000\n",
      "[Epoch:155, Batch:    5] loss: 0.004103\n",
      "[Epoch:155] Test Loss: 0.137366, MAPE: 1193208840192.000\n",
      "[Epoch:156, Batch:    5] loss: 0.004972\n",
      "[Epoch:156] Test Loss: 0.137366, MAPE: 1193249734656.000\n",
      "[Epoch:157, Batch:    5] loss: 0.005380\n",
      "[Epoch:157] Test Loss: 0.137366, MAPE: 1193254584320.000\n",
      "[Epoch:158, Batch:    5] loss: 0.005860\n",
      "[Epoch:158] Test Loss: 0.137366, MAPE: 1193194160128.000\n",
      "[Epoch:159, Batch:    5] loss: 0.005273\n",
      "[Epoch:159] Test Loss: 0.137366, MAPE: 1193088516096.000\n",
      "[Epoch:160, Batch:    5] loss: 0.006160\n",
      "[Epoch:160] Test Loss: 0.137366, MAPE: 1193422618624.000\n",
      "[Epoch:161, Batch:    5] loss: 0.003513\n",
      "[Epoch:161] Test Loss: 0.137366, MAPE: 1193484877824.000\n",
      "[Epoch:162, Batch:    5] loss: 0.004373\n",
      "[Epoch:162] Test Loss: 0.137366, MAPE: 1193422356480.000\n",
      "[Epoch:163, Batch:    5] loss: 0.004682\n",
      "[Epoch:163] Test Loss: 0.137366, MAPE: 1193617522688.000\n",
      "[Epoch:164, Batch:    5] loss: 0.003191\n",
      "[Epoch:164] Test Loss: 0.137366, MAPE: 1193312911360.000\n",
      "[Epoch:165, Batch:    5] loss: 0.004690\n",
      "[Epoch:165] Test Loss: 0.137366, MAPE: 1193177645056.000\n",
      "[Epoch:166, Batch:    5] loss: 0.003963\n",
      "[Epoch:166] Test Loss: 0.137366, MAPE: 1193173319680.000\n",
      "[Epoch:167, Batch:    5] loss: 0.004708\n",
      "[Epoch:167] Test Loss: 0.137366, MAPE: 1193114730496.000\n",
      "[Epoch:168, Batch:    5] loss: 0.005906\n",
      "[Epoch:168] Test Loss: 0.137366, MAPE: 1193040936960.000\n",
      "[Epoch:169, Batch:    5] loss: 0.005083\n",
      "[Epoch:169] Test Loss: 0.137366, MAPE: 1193171746816.000\n",
      "[Epoch:170, Batch:    5] loss: 0.005647\n",
      "[Epoch:170] Test Loss: 0.137366, MAPE: 1192990605312.000\n",
      "[Epoch:171, Batch:    5] loss: 0.005217\n",
      "[Epoch:171] Test Loss: 0.137366, MAPE: 1192984838144.000\n",
      "[Epoch:172, Batch:    5] loss: 0.003051\n",
      "[Epoch:172] Test Loss: 0.137366, MAPE: 1193135833088.000\n",
      "[Epoch:173, Batch:    5] loss: 0.005307\n",
      "[Epoch:173] Test Loss: 0.137366, MAPE: 1193186164736.000\n",
      "[Epoch:174, Batch:    5] loss: 0.003674\n",
      "[Epoch:174] Test Loss: 0.137366, MAPE: 1193297575936.000\n",
      "[Epoch:175, Batch:    5] loss: 0.004726\n",
      "[Epoch:175] Test Loss: 0.137366, MAPE: 1193184460800.000\n",
      "[Epoch:176, Batch:    5] loss: 0.004055\n",
      "[Epoch:176] Test Loss: 0.137366, MAPE: 1193284861952.000\n",
      "[Epoch:177, Batch:    5] loss: 0.004010\n",
      "[Epoch:177] Test Loss: 0.137366, MAPE: 1193267298304.000\n",
      "[Epoch:178, Batch:    5] loss: 0.004853\n",
      "[Epoch:178] Test Loss: 0.137366, MAPE: 1193270050816.000\n",
      "[Epoch:179, Batch:    5] loss: 0.004359\n",
      "[Epoch:179] Test Loss: 0.137366, MAPE: 1193163489280.000\n",
      "[Epoch:180, Batch:    5] loss: 0.004285\n",
      "[Epoch:180] Test Loss: 0.137366, MAPE: 1193139503104.000\n",
      "[Epoch:181, Batch:    5] loss: 0.003969\n",
      "[Epoch:181] Test Loss: 0.137366, MAPE: 1193341222912.000\n",
      "[Epoch:182, Batch:    5] loss: 0.005268\n",
      "[Epoch:182] Test Loss: 0.137366, MAPE: 1193415540736.000\n",
      "[Epoch:183, Batch:    5] loss: 0.004124\n",
      "[Epoch:183] Test Loss: 0.137366, MAPE: 1193492480000.000\n",
      "[Epoch:184, Batch:    5] loss: 0.004475\n",
      "[Epoch:184] Test Loss: 0.137366, MAPE: 1193386442752.000\n",
      "[Epoch:185, Batch:    5] loss: 0.004528\n",
      "[Epoch:185] Test Loss: 0.137366, MAPE: 1193471377408.000\n",
      "[Epoch:186, Batch:    5] loss: 0.004816\n",
      "[Epoch:186] Test Loss: 0.137366, MAPE: 1193176596480.000\n",
      "[Epoch:187, Batch:    5] loss: 0.004941\n",
      "[Epoch:187] Test Loss: 0.137366, MAPE: 1193228107776.000\n",
      "[Epoch:188, Batch:    5] loss: 0.003781\n",
      "[Epoch:188] Test Loss: 0.137366, MAPE: 1193271230464.000\n",
      "[Epoch:189, Batch:    5] loss: 0.004996\n",
      "[Epoch:189] Test Loss: 0.137366, MAPE: 1193323659264.000\n",
      "[Epoch:190, Batch:    5] loss: 0.004604\n",
      "[Epoch:190] Test Loss: 0.137366, MAPE: 1193513451520.000\n",
      "[Epoch:191, Batch:    5] loss: 0.003901\n",
      "[Epoch:191] Test Loss: 0.137366, MAPE: 1193457090560.000\n",
      "[Epoch:192, Batch:    5] loss: 0.004218\n",
      "[Epoch:192] Test Loss: 0.137366, MAPE: 1193564569600.000\n",
      "[Epoch:193, Batch:    5] loss: 0.004460\n",
      "[Epoch:193] Test Loss: 0.137366, MAPE: 1193582657536.000\n",
      "[Epoch:194, Batch:    5] loss: 0.005125\n",
      "[Epoch:194] Test Loss: 0.137366, MAPE: 1193488809984.000\n",
      "[Epoch:195, Batch:    5] loss: 0.004434\n",
      "[Epoch:195] Test Loss: 0.137366, MAPE: 1193354199040.000\n",
      "[Epoch:196, Batch:    5] loss: 0.005618\n",
      "[Epoch:196] Test Loss: 0.137366, MAPE: 1193338077184.000\n",
      "[Epoch:197, Batch:    5] loss: 0.004360\n",
      "[Epoch:197] Test Loss: 0.137366, MAPE: 1193253273600.000\n",
      "[Epoch:198, Batch:    5] loss: 0.005374\n",
      "[Epoch:198] Test Loss: 0.137366, MAPE: 1193312518144.000\n",
      "[Epoch:199, Batch:    5] loss: 0.005791\n",
      "[Epoch:199] Test Loss: 0.137366, MAPE: 1193282240512.000\n",
      "[Epoch:200, Batch:    5] loss: 0.005309\n",
      "[Epoch:200] Test Loss: 0.137366, MAPE: 1193293381632.000\n",
      "[Epoch:201, Batch:    5] loss: 0.004390\n",
      "[Epoch:201] Test Loss: 0.137366, MAPE: 1193378316288.000\n",
      "[Epoch:202, Batch:    5] loss: 0.004921\n",
      "[Epoch:202] Test Loss: 0.137366, MAPE: 1193299279872.000\n",
      "[Epoch:203, Batch:    5] loss: 0.004752\n",
      "[Epoch:203] Test Loss: 0.137366, MAPE: 1193333358592.000\n",
      "[Epoch:204, Batch:    5] loss: 0.004640\n",
      "[Epoch:204] Test Loss: 0.137366, MAPE: 1193498509312.000\n",
      "[Epoch:205, Batch:    5] loss: 0.004234\n",
      "[Epoch:205] Test Loss: 0.137366, MAPE: 1193754624000.000\n",
      "[Epoch:206, Batch:    5] loss: 0.005465\n",
      "[Epoch:206] Test Loss: 0.137366, MAPE: 1193897885696.000\n",
      "[Epoch:207, Batch:    5] loss: 0.003958\n",
      "[Epoch:207] Test Loss: 0.137366, MAPE: 1193796698112.000\n",
      "[Epoch:208, Batch:    5] loss: 0.005862\n",
      "[Epoch:208] Test Loss: 0.137366, MAPE: 1193623552000.000\n",
      "[Epoch:209, Batch:    5] loss: 0.005144\n",
      "[Epoch:209] Test Loss: 0.137366, MAPE: 1193476882432.000\n",
      "[Epoch:210, Batch:    5] loss: 0.005177\n",
      "[Epoch:210] Test Loss: 0.137366, MAPE: 1193624600576.000\n",
      "[Epoch:211, Batch:    5] loss: 0.005798\n",
      "[Epoch:211] Test Loss: 0.137366, MAPE: 1193647407104.000\n",
      "[Epoch:212, Batch:    5] loss: 0.005600\n",
      "[Epoch:212] Test Loss: 0.137366, MAPE: 1193895002112.000\n",
      "[Epoch:213, Batch:    5] loss: 0.004606\n",
      "[Epoch:213] Test Loss: 0.137366, MAPE: 1194028433408.000\n",
      "[Epoch:214, Batch:    5] loss: 0.005286\n",
      "[Epoch:214] Test Loss: 0.137366, MAPE: 1193996976128.000\n",
      "[Epoch:215, Batch:    5] loss: 0.003961\n",
      "[Epoch:215] Test Loss: 0.137366, MAPE: 1193808625664.000\n",
      "[Epoch:216, Batch:    5] loss: 0.004885\n",
      "[Epoch:216] Test Loss: 0.137366, MAPE: 1193710452736.000\n",
      "[Epoch:217, Batch:    5] loss: 0.005329\n",
      "[Epoch:217] Test Loss: 0.137366, MAPE: 1193463906304.000\n",
      "[Epoch:218, Batch:    5] loss: 0.005558\n",
      "[Epoch:218] Test Loss: 0.137366, MAPE: 1193399812096.000\n",
      "[Epoch:219, Batch:    5] loss: 0.004550\n",
      "[Epoch:219] Test Loss: 0.137366, MAPE: 1193328377856.000\n",
      "[Epoch:220, Batch:    5] loss: 0.005858\n",
      "[Epoch:220] Test Loss: 0.137366, MAPE: 1193206349824.000\n",
      "[Epoch:221, Batch:    5] loss: 0.005914\n",
      "[Epoch:221] Test Loss: 0.137366, MAPE: 1193269395456.000\n",
      "[Epoch:222, Batch:    5] loss: 0.004433\n",
      "[Epoch:222] Test Loss: 0.137366, MAPE: 1193201893376.000\n",
      "[Epoch:223, Batch:    5] loss: 0.005125\n",
      "[Epoch:223] Test Loss: 0.137366, MAPE: 1193520005120.000\n",
      "[Epoch:224, Batch:    5] loss: 0.004508\n",
      "[Epoch:224] Test Loss: 0.137366, MAPE: 1193702850560.000\n",
      "[Epoch:225, Batch:    5] loss: 0.004383\n",
      "[Epoch:225] Test Loss: 0.137366, MAPE: 1193686728704.000\n",
      "[Epoch:226, Batch:    5] loss: 0.004240\n",
      "[Epoch:226] Test Loss: 0.137366, MAPE: 1193710977024.000\n",
      "[Epoch:227, Batch:    5] loss: 0.005011\n",
      "[Epoch:227] Test Loss: 0.137366, MAPE: 1193546743808.000\n",
      "[Epoch:228, Batch:    5] loss: 0.004326\n",
      "[Epoch:228] Test Loss: 0.137366, MAPE: 1193505980416.000\n",
      "[Epoch:229, Batch:    5] loss: 0.004137\n",
      "[Epoch:229] Test Loss: 0.137366, MAPE: 1193365340160.000\n",
      "[Epoch:230, Batch:    5] loss: 0.005608\n",
      "[Epoch:230] Test Loss: 0.137366, MAPE: 1193430089728.000\n",
      "[Epoch:231, Batch:    5] loss: 0.005844\n",
      "[Epoch:231] Test Loss: 0.137366, MAPE: 1193668509696.000\n",
      "[Epoch:232, Batch:    5] loss: 0.006155\n",
      "[Epoch:232] Test Loss: 0.137366, MAPE: 1193672704000.000\n",
      "[Epoch:233, Batch:    5] loss: 0.005235\n",
      "[Epoch:233] Test Loss: 0.137366, MAPE: 1193800499200.000\n",
      "[Epoch:234, Batch:    5] loss: 0.004430\n",
      "[Epoch:234] Test Loss: 0.137366, MAPE: 1193605857280.000\n",
      "[Epoch:235, Batch:    5] loss: 0.003868\n",
      "[Epoch:235] Test Loss: 0.137366, MAPE: 1193526951936.000\n",
      "[Epoch:236, Batch:    5] loss: 0.005608\n",
      "[Epoch:236] Test Loss: 0.137366, MAPE: 1193692758016.000\n",
      "[Epoch:237, Batch:    5] loss: 0.005076\n",
      "[Epoch:237] Test Loss: 0.137366, MAPE: 1193664446464.000\n",
      "[Epoch:238, Batch:    5] loss: 0.004560\n",
      "[Epoch:238] Test Loss: 0.137366, MAPE: 1193613721600.000\n",
      "[Epoch:239, Batch:    5] loss: 0.006060\n",
      "[Epoch:239] Test Loss: 0.137366, MAPE: 1193537568768.000\n",
      "[Epoch:240, Batch:    5] loss: 0.005659\n",
      "[Epoch:240] Test Loss: 0.137366, MAPE: 1193549627392.000\n",
      "[Epoch:241, Batch:    5] loss: 0.005528\n",
      "[Epoch:241] Test Loss: 0.137366, MAPE: 1193468231680.000\n",
      "[Epoch:242, Batch:    5] loss: 0.005705\n",
      "[Epoch:242] Test Loss: 0.137366, MAPE: 1193663791104.000\n",
      "[Epoch:243, Batch:    5] loss: 0.004212\n",
      "[Epoch:243] Test Loss: 0.137366, MAPE: 1193699573760.000\n",
      "[Epoch:244, Batch:    5] loss: 0.005567\n",
      "[Epoch:244] Test Loss: 0.137366, MAPE: 1193389719552.000\n",
      "[Epoch:245, Batch:    5] loss: 0.006424\n",
      "[Epoch:245] Test Loss: 0.137366, MAPE: 1193618440192.000\n",
      "[Epoch:246, Batch:    5] loss: 0.004190\n",
      "[Epoch:246] Test Loss: 0.137366, MAPE: 1193664184320.000\n",
      "[Epoch:247, Batch:    5] loss: 0.004912\n",
      "[Epoch:247] Test Loss: 0.137366, MAPE: 1193589866496.000\n",
      "[Epoch:248, Batch:    5] loss: 0.004874\n",
      "[Epoch:248] Test Loss: 0.137366, MAPE: 1193413050368.000\n",
      "[Epoch:249, Batch:    5] loss: 0.003891\n",
      "[Epoch:249] Test Loss: 0.137366, MAPE: 1193373990912.000\n",
      "[Epoch:250, Batch:    5] loss: 0.004469\n",
      "[Epoch:250] Test Loss: 0.137366, MAPE: 1193425371136.000\n",
      "[Epoch:251, Batch:    5] loss: 0.005867\n",
      "[Epoch:251] Test Loss: 0.137366, MAPE: 1193249341440.000\n",
      "[Epoch:252, Batch:    5] loss: 0.005140\n",
      "[Epoch:252] Test Loss: 0.137366, MAPE: 1193135308800.000\n",
      "[Epoch:253, Batch:    5] loss: 0.003112\n",
      "[Epoch:253] Test Loss: 0.137366, MAPE: 1193276211200.000\n",
      "[Epoch:254, Batch:    5] loss: 0.005009\n",
      "[Epoch:254] Test Loss: 0.137366, MAPE: 1193436250112.000\n",
      "[Epoch:255, Batch:    5] loss: 0.005107\n",
      "[Epoch:255] Test Loss: 0.137366, MAPE: 1193677553664.000\n",
      "[Epoch:256, Batch:    5] loss: 0.004481\n",
      "[Epoch:256] Test Loss: 0.137366, MAPE: 1193470853120.000\n",
      "[Epoch:257, Batch:    5] loss: 0.005452\n",
      "[Epoch:257] Test Loss: 0.137366, MAPE: 1193557491712.000\n",
      "[Epoch:258, Batch:    5] loss: 0.003809\n",
      "[Epoch:258] Test Loss: 0.137366, MAPE: 1193529311232.000\n",
      "[Epoch:259, Batch:    5] loss: 0.004508\n",
      "[Epoch:259] Test Loss: 0.137366, MAPE: 1193299935232.000\n",
      "[Epoch:260, Batch:    5] loss: 0.004913\n",
      "[Epoch:260] Test Loss: 0.137366, MAPE: 1193362849792.000\n",
      "[Epoch:261, Batch:    5] loss: 0.005875\n",
      "[Epoch:261] Test Loss: 0.137366, MAPE: 1193255501824.000\n",
      "[Epoch:262, Batch:    5] loss: 0.006250\n",
      "[Epoch:262] Test Loss: 0.137366, MAPE: 1193289711616.000\n",
      "[Epoch:263, Batch:    5] loss: 0.004592\n",
      "[Epoch:263] Test Loss: 0.137366, MAPE: 1193232957440.000\n",
      "[Epoch:264, Batch:    5] loss: 0.005285\n",
      "[Epoch:264] Test Loss: 0.137366, MAPE: 1193459843072.000\n",
      "[Epoch:265, Batch:    5] loss: 0.004582\n",
      "[Epoch:265] Test Loss: 0.137366, MAPE: 1193532063744.000\n",
      "[Epoch:266, Batch:    5] loss: 0.005041\n",
      "[Epoch:266] Test Loss: 0.137366, MAPE: 1193659203584.000\n",
      "[Epoch:267, Batch:    5] loss: 0.004032\n",
      "[Epoch:267] Test Loss: 0.137366, MAPE: 1193758687232.000\n",
      "[Epoch:268, Batch:    5] loss: 0.004586\n",
      "[Epoch:268] Test Loss: 0.137366, MAPE: 1194054647808.000\n",
      "[Epoch:269, Batch:    5] loss: 0.004156\n",
      "[Epoch:269] Test Loss: 0.137366, MAPE: 1193687646208.000\n",
      "[Epoch:270, Batch:    5] loss: 0.004217\n",
      "[Epoch:270] Test Loss: 0.137366, MAPE: 1193899851776.000\n",
      "[Epoch:271, Batch:    5] loss: 0.003977\n",
      "[Epoch:271] Test Loss: 0.137366, MAPE: 1193911386112.000\n",
      "[Epoch:272, Batch:    5] loss: 0.005340\n",
      "[Epoch:272] Test Loss: 0.137366, MAPE: 1193608740864.000\n",
      "[Epoch:273, Batch:    5] loss: 0.003712\n",
      "[Epoch:273] Test Loss: 0.137366, MAPE: 1193450274816.000\n",
      "[Epoch:274, Batch:    5] loss: 0.005972\n",
      "[Epoch:274] Test Loss: 0.137366, MAPE: 1193514237952.000\n",
      "[Epoch:275, Batch:    5] loss: 0.005453\n",
      "[Epoch:275] Test Loss: 0.137366, MAPE: 1193583837184.000\n",
      "[Epoch:276, Batch:    5] loss: 0.005141\n",
      "[Epoch:276] Test Loss: 0.137366, MAPE: 1193639542784.000\n",
      "[Epoch:277, Batch:    5] loss: 0.005456\n",
      "[Epoch:277] Test Loss: 0.137366, MAPE: 1193692233728.000\n",
      "[Epoch:278, Batch:    5] loss: 0.005225\n",
      "[Epoch:278] Test Loss: 0.137366, MAPE: 1193488678912.000\n",
      "[Epoch:279, Batch:    5] loss: 0.004370\n",
      "[Epoch:279] Test Loss: 0.137366, MAPE: 1193579773952.000\n",
      "[Epoch:280, Batch:    5] loss: 0.005562\n",
      "[Epoch:280] Test Loss: 0.137366, MAPE: 1193634824192.000\n",
      "[Epoch:281, Batch:    5] loss: 0.004949\n",
      "[Epoch:281] Test Loss: 0.137366, MAPE: 1193669820416.000\n",
      "[Epoch:282, Batch:    5] loss: 0.004055\n",
      "[Epoch:282] Test Loss: 0.137366, MAPE: 1193599172608.000\n",
      "[Epoch:283, Batch:    5] loss: 0.004457\n",
      "[Epoch:283] Test Loss: 0.137366, MAPE: 1193591439360.000\n",
      "[Epoch:284, Batch:    5] loss: 0.003974\n",
      "[Epoch:284] Test Loss: 0.137366, MAPE: 1193486319616.000\n",
      "[Epoch:285, Batch:    5] loss: 0.006102\n",
      "[Epoch:285] Test Loss: 0.137366, MAPE: 1193553166336.000\n",
      "[Epoch:286, Batch:    5] loss: 0.006134\n",
      "[Epoch:286] Test Loss: 0.137366, MAPE: 1193745055744.000\n",
      "[Epoch:287, Batch:    5] loss: 0.004028\n",
      "[Epoch:287] Test Loss: 0.137366, MAPE: 1193691578368.000\n",
      "[Epoch:288, Batch:    5] loss: 0.005626\n",
      "[Epoch:288] Test Loss: 0.137366, MAPE: 1193735356416.000\n",
      "[Epoch:289, Batch:    5] loss: 0.004718\n",
      "[Epoch:289] Test Loss: 0.137366, MAPE: 1193888972800.000\n",
      "[Epoch:290, Batch:    5] loss: 0.004447\n",
      "[Epoch:290] Test Loss: 0.137366, MAPE: 1193902080000.000\n",
      "[Epoch:291, Batch:    5] loss: 0.005943\n",
      "[Epoch:291] Test Loss: 0.137366, MAPE: 1193690136576.000\n",
      "[Epoch:292, Batch:    5] loss: 0.004802\n",
      "[Epoch:292] Test Loss: 0.137366, MAPE: 1193731031040.000\n",
      "[Epoch:293, Batch:    5] loss: 0.005546\n",
      "[Epoch:293] Test Loss: 0.137366, MAPE: 1193640329216.000\n",
      "[Epoch:294, Batch:    5] loss: 0.004954\n",
      "[Epoch:294] Test Loss: 0.137366, MAPE: 1193858564096.000\n",
      "[Epoch:295, Batch:    5] loss: 0.004420\n",
      "[Epoch:295] Test Loss: 0.137366, MAPE: 1193643999232.000\n",
      "[Epoch:296, Batch:    5] loss: 0.004134\n",
      "[Epoch:296] Test Loss: 0.137366, MAPE: 1193531539456.000\n",
      "[Epoch:297, Batch:    5] loss: 0.004357\n",
      "[Epoch:297] Test Loss: 0.137366, MAPE: 1193631940608.000\n",
      "[Epoch:298, Batch:    5] loss: 0.005565\n",
      "[Epoch:298] Test Loss: 0.137366, MAPE: 1193801023488.000\n",
      "[Epoch:299, Batch:    5] loss: 0.005435\n",
      "[Epoch:299] Test Loss: 0.137366, MAPE: 1193674932224.000\n",
      "[Epoch:300, Batch:    5] loss: 0.004116\n",
      "[Epoch:300] Test Loss: 0.137366, MAPE: 1193701146624.000\n",
      "[Epoch:301, Batch:    5] loss: 0.004938\n",
      "[Epoch:301] Test Loss: 0.137366, MAPE: 1193656844288.000\n",
      "[Epoch:302, Batch:    5] loss: 0.004455\n",
      "[Epoch:302] Test Loss: 0.137366, MAPE: 1193444769792.000\n",
      "[Epoch:303, Batch:    5] loss: 0.004074\n",
      "[Epoch:303] Test Loss: 0.137366, MAPE: 1193614639104.000\n",
      "[Epoch:304, Batch:    5] loss: 0.004187\n",
      "[Epoch:304] Test Loss: 0.137366, MAPE: 1193666543616.000\n",
      "[Epoch:305, Batch:    5] loss: 0.004213\n",
      "[Epoch:305] Test Loss: 0.137366, MAPE: 1193822388224.000\n",
      "[Epoch:306, Batch:    5] loss: 0.004125\n",
      "[Epoch:306] Test Loss: 0.137366, MAPE: 1193802727424.000\n",
      "[Epoch:307, Batch:    5] loss: 0.005001\n",
      "[Epoch:307] Test Loss: 0.137366, MAPE: 1193862758400.000\n",
      "[Epoch:308, Batch:    5] loss: 0.004998\n",
      "[Epoch:308] Test Loss: 0.137366, MAPE: 1193762357248.000\n",
      "[Epoch:309, Batch:    5] loss: 0.005488\n",
      "[Epoch:309] Test Loss: 0.137366, MAPE: 1193668509696.000\n",
      "[Epoch:310, Batch:    5] loss: 0.004835\n",
      "[Epoch:310] Test Loss: 0.137366, MAPE: 1193660252160.000\n",
      "[Epoch:311, Batch:    5] loss: 0.004135\n",
      "[Epoch:311] Test Loss: 0.137366, MAPE: 1193536389120.000\n",
      "[Epoch:312, Batch:    5] loss: 0.005280\n",
      "[Epoch:312] Test Loss: 0.137366, MAPE: 1193426419712.000\n",
      "[Epoch:313, Batch:    5] loss: 0.004635\n",
      "[Epoch:313] Test Loss: 0.137366, MAPE: 1193523806208.000\n",
      "[Epoch:314, Batch:    5] loss: 0.004200\n",
      "[Epoch:314] Test Loss: 0.137366, MAPE: 1193621848064.000\n",
      "[Epoch:315, Batch:    5] loss: 0.003895\n",
      "[Epoch:315] Test Loss: 0.137366, MAPE: 1193566011392.000\n",
      "[Epoch:316, Batch:    5] loss: 0.005175\n",
      "[Epoch:316] Test Loss: 0.137366, MAPE: 1193638232064.000\n",
      "[Epoch:317, Batch:    5] loss: 0.004412\n",
      "[Epoch:317] Test Loss: 0.137366, MAPE: 1193599696896.000\n",
      "[Epoch:318, Batch:    5] loss: 0.005869\n",
      "[Epoch:318] Test Loss: 0.137366, MAPE: 1193594060800.000\n",
      "[Epoch:319, Batch:    5] loss: 0.005827\n",
      "[Epoch:319] Test Loss: 0.137366, MAPE: 1193527869440.000\n",
      "[Epoch:320, Batch:    5] loss: 0.004795\n",
      "[Epoch:320] Test Loss: 0.137366, MAPE: 1193712812032.000\n",
      "[Epoch:321, Batch:    5] loss: 0.005904\n",
      "[Epoch:321] Test Loss: 0.137366, MAPE: 1193702326272.000\n",
      "[Epoch:322, Batch:    5] loss: 0.004749\n",
      "[Epoch:322] Test Loss: 0.137366, MAPE: 1193817669632.000\n",
      "[Epoch:323, Batch:    5] loss: 0.003979\n",
      "[Epoch:323] Test Loss: 0.137366, MAPE: 1193661956096.000\n",
      "[Epoch:324, Batch:    5] loss: 0.005064\n",
      "[Epoch:324] Test Loss: 0.137366, MAPE: 1193801678848.000\n",
      "[Epoch:325, Batch:    5] loss: 0.005166\n",
      "[Epoch:325] Test Loss: 0.137366, MAPE: 1193898409984.000\n",
      "[Epoch:326, Batch:    5] loss: 0.003587\n",
      "[Epoch:326] Test Loss: 0.137366, MAPE: 1193910468608.000\n",
      "[Epoch:327, Batch:    5] loss: 0.004783\n",
      "[Epoch:327] Test Loss: 0.137366, MAPE: 1193660907520.000\n",
      "[Epoch:328, Batch:    5] loss: 0.005467\n",
      "[Epoch:328] Test Loss: 0.137366, MAPE: 1193617784832.000\n",
      "[Epoch:329, Batch:    5] loss: 0.004427\n",
      "[Epoch:329] Test Loss: 0.137366, MAPE: 1193840607232.000\n",
      "[Epoch:330, Batch:    5] loss: 0.003772\n",
      "[Epoch:330] Test Loss: 0.137366, MAPE: 1193899851776.000\n",
      "[Epoch:331, Batch:    5] loss: 0.005520\n",
      "[Epoch:331] Test Loss: 0.137366, MAPE: 1194077585408.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 266\u001b[0m\n\u001b[1;32m    263\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m'''#Example usage:\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mmodel = Model()\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m# Output will have shape [1, 3, 480, 480]\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03mprint(output.shape)'''\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 217\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[1;32m    216\u001b[0m e_loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m--> 217\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43me_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Store predictions and targets for MAPE calculation\u001b[39;00m\n\u001b[1;32m    220\u001b[0m all_predictions\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "\n",
    "        # Define ResNet layers (modify based on your specific ResNet architecture)\n",
    "        self.resnet = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            # Add more ResNet layers here...\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "class FNNReducer(nn.Module):\n",
    "    def __init__(self, input_size):  # Pass the input size here\n",
    "        super(FNNReducer, self).__init__()\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_size, out_features=1024),  # Use input_size\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 20 * 20),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class CNNExpander(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNExpander, self).__init__()\n",
    "\n",
    "        # Define convolutional layers\n",
    "        self.conv_transpose = nn.ConvTranspose2d(\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        kernel_size=(24, 24),\n",
    "        stride=(24, 24),\n",
    "        padding=(0, 0)\n",
    "    )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_transpose(x)\n",
    "        return x\n",
    "\n",
    "class UpsampleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UpsampleNet, self).__init__()\n",
    "        # Define the first ConvTranspose2d layer\n",
    "        self.conv_transpose1 = nn.ConvTranspose2d(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            output_padding=1\n",
    "        )\n",
    "        # Define the second ConvTranspose2d layer\n",
    "        self.conv_transpose2 = nn.ConvTranspose2d(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            output_padding=1\n",
    "        )\n",
    "        # ... Add more layers as needed ...\n",
    "        # Define the final ConvTranspose2d layer to reach the desired size\n",
    "        self.conv_transpose_final = nn.ConvTranspose2d(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "            stride=24,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the first ConvTranspose2d layer\n",
    "        x = self.conv_transpose1(x)\n",
    "        # Apply the second ConvTranspose2d layer\n",
    "        x = self.conv_transpose2(x)\n",
    "        # ... Apply additional layers as needed ...\n",
    "        # Apply the final ConvTranspose2d layer\n",
    "        x = self.conv_transpose_final(x)\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.resnet_encoder = ResNetEncoder()\n",
    "        self.resnet_encoder2 = ResNetEncoder()\n",
    "        # Calculate the input size for FNNReducer after a forward pass through ResNetEncoder\n",
    "        dummy_input = torch.randn(1, 1, 480, 480)\n",
    "        dummy_output = self.resnet_encoder(dummy_input)\n",
    "        # Fix: Calculate input size considering the concatenation of two ResNet outputs\n",
    "        input_size = dummy_output.shape[1] * 2 * dummy_output.shape[2] * dummy_output.shape[3]\n",
    "        self.fnn_reducer = FNNReducer(input_size)  # Pass the calculated input size\n",
    "        self.cnn_expander = CNNExpander()\n",
    "        self.UpsampleNet= UpsampleNet()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # ResNet encoding\n",
    "        x1 = self.resnet_encoder(x1)\n",
    "        x2 = self.resnet_encoder2(x2)\n",
    "\n",
    "        # Concatenate features\n",
    "        x = torch.cat((x1, x2), dim=1)  # Concatenation doubles the number of channels\n",
    "\n",
    "        # FNN reduction\n",
    "        x = self.fnn_reducer(x)\n",
    "\n",
    "        # Reshape to [batch_size, 1, 20, 20]\n",
    "        x = x.view(-1, 1, 20, 20)\n",
    "        #print(x.shape)\n",
    "        # CNN expansion\n",
    "        x = self.cnn_expander(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, input1_folder, input2_folder, output_folder, transform=None):\n",
    "        self.input1_folder = input1_folder\n",
    "        self.input2_folder = input2_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.input1_filenames = sorted(os.listdir(input1_folder))\n",
    "        self.input2_filenames = sorted(os.listdir(input2_folder))\n",
    "        self.output_filenames = sorted(os.listdir(output_folder))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input1_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input1_path = os.path.join(self.input1_folder, self.input1_filenames[idx])\n",
    "        input2_path = os.path.join(self.input2_folder, self.input2_filenames[idx])\n",
    "        output_path = os.path.join(self.output_folder, self.output_filenames[idx])\n",
    "\n",
    "        input1 = Image.open(input1_path).convert('L')\n",
    "        input2 = Image.open(input2_path).convert('L')\n",
    "        output = Image.open(output_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            input1 = self.transform(input1)\n",
    "            input2 = self.transform(input2)\n",
    "            output = self.transform(output)\n",
    "\n",
    "        return input1, input2, output\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, optimizer, criterion, epochs):\n",
    "# CUDA Usage:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Model().to(device)  # Move the model to the GPU\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()  # Set the model to training mode\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            input1, input2, target = data\n",
    "            input1 = input1.to(device)\n",
    "            input2 = input2.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input1, input2)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 4:  # Print statistics every 4 batches\n",
    "                print(f'[Epoch:{epoch + 1}, Batch:{i + 1:5d}] loss: {running_loss / 100:.6f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "             # Evaluation phase (after each epoch)\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                test_loss = 0.0\n",
    "                all_predictions = []\n",
    "                all_targets = []\n",
    "                for input1, input2, target in test_dataloader:\n",
    "                    input1 = input1.to(device)\n",
    "                    input2 = input2.to(device)\n",
    "                    target = target.to(device)\n",
    "\n",
    "                    # Predict using the model\n",
    "                    output = model(input1, input2)\n",
    "\n",
    "                    # Calculate the loss\n",
    "                    e_loss = criterion(output, target)\n",
    "                    test_loss += e_loss.item()\n",
    "\n",
    "                    # Store predictions and targets for MAPE calculation\n",
    "                    all_predictions.append(output.cpu().numpy())\n",
    "                    all_targets.append(target.cpu().numpy())\n",
    "\n",
    "                # Calculate MAPE\n",
    "                all_predictions = np.concatenate(all_predictions).flatten() # Flatten the array\n",
    "                all_targets = np.concatenate(all_targets).flatten() # Flatten the array\n",
    "                mape = mean_absolute_percentage_error(all_targets, all_predictions)\n",
    "\n",
    "                # Print evaluation results\n",
    "                if i % 100 == 4:  # Print statistics every 4 batches\n",
    "                  print(f'[Epoch:{epoch + 1}] Test Loss: {test_loss / len(test_dataloader):.6f}, MAPE: {mape:.3f}')\n",
    "\n",
    "\n",
    "# Define the training function (same as before)\n",
    "# ... (code for train)\n",
    "\n",
    "\n",
    "\n",
    "# Set paths to your input and output folders\n",
    "input1_folder = '/workspace/AI-Thermal'  # Replace with your actual path\n",
    "input2_folder = '/workspace/AI-EC/EC-5.0'  # Replace with your actual path\n",
    "output_folder = '/workspace/AI-CH4 flux' # Replace with your actual path\n",
    "\n",
    "# Define data transformations (e.g., normalization, resizing)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((480, 480)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageDataset(input1_folder, input2_folder, output_folder, transform=transform)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Use batch size 1 for test\n",
    "\n",
    "\n",
    "# Create the model, optimizer, and loss function\n",
    "model = Model()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "train(model, train_dataloader, test_dataloader, optimizer, criterion, epochs=500)\n",
    "\n",
    "'''#Example usage:\n",
    "model = Model()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Input images (assume these are your 2-channel images)\n",
    "image1 = torch.randn(1, 1, 480, 480)\n",
    "image2 = torch.randn(1, 1, 480, 480)\n",
    "\n",
    "# Pass images through the model\n",
    "output = model(image1, image2)\n",
    "\n",
    "# Output will have shape [1, 3, 480, 480]\n",
    "print(output.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [1, 1, 480, 480]          30\n",
       "ResNetEncoder: 1-1                     [1, 64, 120, 120]         --\n",
       "    Sequential: 2-1                   [1, 64, 120, 120]         --\n",
       "        Conv2d: 3-1                  [1, 64, 240, 240]         3,136\n",
       "        BatchNorm2d: 3-2             [1, 64, 240, 240]         128\n",
       "        ReLU: 3-3                    [1, 64, 240, 240]         --\n",
       "        MaxPool2d: 3-4               [1, 64, 120, 120]         --\n",
       "ResNetEncoder: 1-2                     [1, 64, 120, 120]         --\n",
       "    Sequential: 2-2                   [1, 64, 120, 120]         --\n",
       "        Conv2d: 3-5                  [1, 64, 240, 240]         3,136\n",
       "        BatchNorm2d: 3-6             [1, 64, 240, 240]         128\n",
       "        ReLU: 3-7                    [1, 64, 240, 240]         --\n",
       "        MaxPool2d: 3-8               [1, 64, 120, 120]         --\n",
       "FNNReducer: 1-3                        [1, 400]                  --\n",
       "    Sequential: 2-3                   [1, 400]                  --\n",
       "        Flatten: 3-9                 [1, 1843200]              --\n",
       "        Linear: 3-10                 [1, 1024]                 1,887,437,824\n",
       "        ReLU: 3-11                   [1, 1024]                 --\n",
       "        Linear: 3-12                 [1, 400]                  410,000\n",
       "        Sigmoid: 3-13                [1, 400]                  --\n",
       "CNNExpander: 1-4                       [1, 1, 480, 480]          --\n",
       "    ConvTranspose2d: 2-4              [1, 1, 480, 480]          577\n",
       "==========================================================================================\n",
       "Total params: 1,887,854,959\n",
       "Trainable params: 1,887,854,959\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.38\n",
       "==========================================================================================\n",
       "Input size (MB): 1.84\n",
       "Forward/backward pass size (MB): 119.82\n",
       "Params size (MB): 7551.42\n",
       "Estimated Total Size (MB): 7673.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print a detailed summary of the model\n",
    "summary(model, input_size=[(1, 1, 480, 480), (1, 1, 480, 480)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8WysMDADrqk",
    "outputId": "70e86a0d-8568-40ff-f980-10562120360f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (10.0.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.3)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.17.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m41.1/41.1 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.1 scipy-1.14.0 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch Pillow numpy torchvision scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-Nuc8waQY9En"
   },
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file '/workspace/AI-Thermal/.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 312\u001b[0m\n\u001b[1;32m    308\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ImageDataset(input1_folder, input2_folder, input3_folder,\n\u001b[1;32m    309\u001b[0m                        output_folder, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Split the dataset into train and test sets\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m train_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# Create dataloaders\u001b[39;00m\n\u001b[1;32m    315\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2805\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2801\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[1;32m   2803\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[0;32m-> 2805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2807\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\n\u001b[1;32m   2808\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2809\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2807\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2801\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[1;32m   2803\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[1;32m   2805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2806\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m-> 2807\u001b[0m         (_safe_indexing(a, train), \u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2808\u001b[0m     )\n\u001b[1;32m   2809\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_indexing.py:269\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_list_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_indexing.py:60\u001b[0m, in \u001b[0;36m_list_indexing\u001b[0;34m(X, key, key_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compress(X, key))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# key is a integer array-like of key\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [X[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m key]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_indexing.py:60\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compress(X, key))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# key is a integer array-like of key\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m key]\n",
      "Cell \u001b[0;32mIn[2], line 210\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    207\u001b[0m input3_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput3_folder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput3_filenames[idx])\n\u001b[1;32m    208\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_folder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_filenames[idx])\n\u001b[0;32m--> 210\u001b[0m input1 \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    211\u001b[0m input2 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(input2_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    212\u001b[0m input3 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(input3_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3280\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3278\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[1;32m   3279\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[0;32m-> 3280\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '/workspace/AI-Thermal/.DS_Store'"
     ]
    }
   ],
   "source": [
    "#code for 3 input picture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "class ResNet50Encoder(nn.Module):\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(ResNet50Encoder, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet50 model\n",
    "        self.resnet50 = resnet50(pretrained=True)\n",
    "        \n",
    "        # Replace the first convolution layer to accept a different number of channels\n",
    "        self.resnet50.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Remove the fully connected layer and the average pooling layer\n",
    "        self.resnet50 = nn.Sequential(*list(self.resnet50.children())[:-2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x)\n",
    "        return x\n",
    "        \n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "\n",
    "        # Define ResNet layers (modify based on your specific ResNet architecture)\n",
    "        self.resnet = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=20, stride=5, padding=1),\n",
    "            # Add more ResNet layers here...\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "class FNNReducer(nn.Module):\n",
    "    def __init__(self, input_size):  # Pass the input size here\n",
    "        super(FNNReducer, self).__init__()\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_size, out_features=1024),  # Use input_size\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 20 * 20),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class CNNExpander(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNExpander, self).__init__()\n",
    "\n",
    "        # Define convolutional layers\n",
    "        self.conv_transpose = nn.ConvTranspose2d(\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        kernel_size=(24, 24),\n",
    "        stride=(24, 24),\n",
    "        padding=(0, 0)\n",
    "    )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_transpose(x)\n",
    "        return x\n",
    "\n",
    "class UpsampleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UpsampleNet, self).__init__()\n",
    "        # Define the first ConvTranspose2d layer\n",
    "        self.conv_transpose1 = nn.ConvTranspose2d(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            output_padding=1\n",
    "        )\n",
    "        # Define the second ConvTranspose2d layer\n",
    "        self.conv_transpose2 = nn.ConvTranspose2d(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            output_padding=1\n",
    "        )\n",
    "        # ... Add more layers as needed ...\n",
    "        # Define the final ConvTranspose2d layer to reach the desired size\n",
    "        self.conv_transpose_final = nn.ConvTranspose2d(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "            stride=24,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the first ConvTranspose2d layer\n",
    "        x = self.conv_transpose1(x)\n",
    "        # Apply the second ConvTranspose2d layer\n",
    "        x = self.conv_transpose2(x)\n",
    "        # ... Apply additional layers as needed ...\n",
    "        # Apply the final ConvTranspose2d layer\n",
    "        x = self.conv_transpose_final(x)\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.resnet_encoder1 = ResNet50Encoder()\n",
    "        self.resnet_encoder2 = ResNet50Encoder()\n",
    "        self.resnet_encoder3 = ResNet50Encoder()\n",
    "        # Calculate the input size for FNNReducer after a forward pass through ResNetEncoder\n",
    "        dummy_input = torch.randn(1, 1, 480, 480)\n",
    "        dummy_output = self.resnet_encoder1(dummy_input)\n",
    "        # Fix: Calculate input size considering the concatenation of three ResNet outputs\n",
    "        input_size = dummy_output.shape[1] * 3 * dummy_output.shape[2] * dummy_output.shape[3]\n",
    "        self.fnn_reducer = FNNReducer(input_size)  # Pass the calculated input size\n",
    "        self.cnn_expander = CNNExpander()\n",
    "        self.UpsampleNet= UpsampleNet()\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        # ResNet encoding\n",
    "        x1 = self.resnet_encoder1(x1)\n",
    "        x2 = self.resnet_encoder2(x2)\n",
    "        x3 = self.resnet_encoder3(x3)\n",
    "\n",
    "\n",
    "        # Concatenate features\n",
    "        x = torch.cat((x1, x2,x3), dim=1)  # Concatenation doubles the number of channels\n",
    "\n",
    "        # FNN reduction\n",
    "        x = self.fnn_reducer(x)\n",
    "\n",
    "        # Reshape to [batch_size, 1, 20, 20]\n",
    "        x = x.view(-1, 1, 20, 20)\n",
    "        #print(x.shape)\n",
    "        # CNN expansion\n",
    "        x = self.cnn_expander(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, input1_folder, input2_folder, output_folder, transform=None):\n",
    "        self.input1_folder = input1_folder\n",
    "        self.input2_folder = input2_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.input1_filenames = sorted(os.listdir(input1_folder))\n",
    "        self.input2_filenames = sorted(os.listdir(input2_folder))\n",
    "        self.output_filenames = sorted(os.listdir(output_folder))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input1_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input1_path = os.path.join(self.input1_folder, self.input1_filenames[idx])\n",
    "        input2_path = os.path.join(self.input2_folder, self.input2_filenames[idx])\n",
    "        output_path = os.path.join(self.output_folder, self.output_filenames[idx])\n",
    "\n",
    "        input1 = Image.open(input1_path).convert('L')\n",
    "        input2 = Image.open(input2_path).convert('L')\n",
    "        output = Image.open(output_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            input1 = self.transform(input1)\n",
    "            input2 = self.transform(input2)\n",
    "            output = self.transform(output)\n",
    "\n",
    "        return input1, input2, output\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, input1_folder, input2_folder, input3_folder,output_folder, transform=None):\n",
    "        self.input1_folder = input1_folder\n",
    "        self.input2_folder = input2_folder\n",
    "        self.input3_folder = input3_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.input1_filenames = sorted(os.listdir(input1_folder))\n",
    "        self.input2_filenames = sorted(os.listdir(input2_folder))\n",
    "        self.input3_filenames = sorted(os.listdir(input3_folder))\n",
    "        self.output_filenames = sorted(os.listdir(output_folder))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input1_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input1_path = os.path.join(self.input1_folder, self.input1_filenames[idx])\n",
    "        input2_path = os.path.join(self.input2_folder, self.input2_filenames[idx])\n",
    "        input3_path = os.path.join(self.input3_folder, self.input3_filenames[idx])\n",
    "        output_path = os.path.join(self.output_folder, self.output_filenames[idx])\n",
    "\n",
    "        input1 = Image.open(input1_path).convert('L')\n",
    "        input2 = Image.open(input2_path).convert('L')\n",
    "        input3 = Image.open(input3_path).convert('L')\n",
    "        output = Image.open(output_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            input1 = self.transform(input1)\n",
    "            input2 = self.transform(input2)\n",
    "            input3 = self.transform(input3)\n",
    "            output = self.transform(output)\n",
    "\n",
    "        return input1, input2,  input3, output\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, optimizer, criterion, epochs):\n",
    "# CUDA Usage:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Model().to(device)  # Move the model to the GPU\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()  # Set the model to training mode\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            input1, input2, input3, target = data\n",
    "            input1 = input1.to(device)\n",
    "            input2 = input2.to(device)\n",
    "            input3 = input3.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input1, input2,input3)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 2:  # Print statistics every 4 batches\n",
    "                print(f'[Epoch:{epoch + 1}, Batch:{i + 1:5d}] loss: {running_loss / 100:.6f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "             # Evaluation phase (after each epoch)\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                test_loss = 0.0\n",
    "                all_predictions = []\n",
    "                all_targets = []\n",
    "                for input1, input2, input3, target in test_dataloader:\n",
    "                    input1 = input1.to(device)\n",
    "                    input2 = input2.to(device)\n",
    "                    input3 = input3.to(device)\n",
    "                    target = target.to(device)\n",
    "\n",
    "                    # Predict using the model\n",
    "                    output = model(input1, input2,input3)\n",
    "\n",
    "                    # Calculate the loss\n",
    "                    loss = criterion(output, target)\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                    # Store predictions and targets for MAPE calculation\n",
    "                    all_predictions.append(output.cpu().numpy())\n",
    "                    all_targets.append(target.cpu().numpy())\n",
    "\n",
    "                # Calculate MAPE\n",
    "                all_predictions = np.concatenate(all_predictions).flatten() # Flatten the array\n",
    "                all_targets = np.concatenate(all_targets).flatten() # Flatten the array\n",
    "                mape = mean_absolute_percentage_error(all_targets, all_predictions)\n",
    "\n",
    "                # Print evaluation results\n",
    "                if i % 100 == 2:  # Print statistics every 4 batches\n",
    "                  print(f'[Epoch:{epoch + 1}] Test Loss: {test_loss / len(test_dataloader):.6f}, MAPE: {mape:.3f}')\n",
    "\n",
    "\n",
    "# Define the training function (same as before)\n",
    "# ... (code for train)\n",
    "\n",
    "\n",
    "\n",
    "# Set paths to your input and output folders\n",
    "input1_folder = '/workspace/AI-Thermal'  # Replace with your actual path\n",
    "input2_folder = '/workspace/AI-EC/EC-5.0'  # Replace with your actual path\n",
    "input3_folder = '/workspace/AI-EC/EC-1.5'  # Replace with your actual path\n",
    "output_folder = '/workspace/AI-CH4 flux' # Replace with your actual path\n",
    "\n",
    "# Define data transformations (e.g., normalization, resizing)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((480, 480)),\n",
    "    transforms.ToTensor()\n",
    "    \n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageDataset(input1_folder, input2_folder, input3_folder,\n",
    "                       output_folder, transform=transform)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Use batch size 1 for test\n",
    "\n",
    "\n",
    "# Create the model, optimizer, and loss function\n",
    "model = Model()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "train(model, train_dataloader, test_dataloader, optimizer, criterion, epochs=500)\n",
    "\n",
    "'''\n",
    "model = Model()\n",
    "# Input images (assume these are your 2-channel images)\n",
    "image1 = torch.randn(1, 1, 480, 480)\n",
    "image2 = torch.randn(1, 1, 480, 480)\n",
    "image3 = torch.randn(1, 1, 480, 480)\n",
    "\n",
    "# Pass images through the model\n",
    "output = model(image1, image2,image3)\n",
    "\n",
    "# Output will have shape [1, 3, 480, 480]\n",
    "print(output.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPelaLvl4dde"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
